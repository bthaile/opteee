name: Deploy to Hugging Face Space

on:
  push:
    branches: [ main ]
  workflow_dispatch:  # Allows manual triggering

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 1  # Only fetch the latest commit

      - name: Configure Git
        run: |
          git config --global user.email "github-actions@github.com"
          git config --global user.name "GitHub Actions"
          # Set default branch name to main
          git config --global init.defaultBranch main
          
      - name: Create Hugging Face specific files
        run: |
          # Create packages.txt for system dependencies
          echo "" > packages.txt
          
          # Create requirements.txt specifically for Hugging Face
          cat > requirements.txt <<EOF
          # Pin specific versions that work together
          torch==1.13.1
          transformers==4.26.0
          tokenizers==0.13.3
          huggingface_hub>=0.14.0
          # Vector store and other requirements
          faiss-cpu
          python-dotenv
          sentence_transformers>=2.2.2
          gradio>=3.20.0
          # Add other dependencies below but don't specify versions unless needed
          EOF
          
          # Create a modernized version of create_vector_store.py 
          cat > create_vector_store.py <<'EOF'
          import os
          import json
          import numpy as np
          from tqdm import tqdm
          import faiss
          import pickle
          from sentence_transformers import SentenceTransformer
          import argparse

          # Configuration
          PROCESSED_DIR = "processed_transcripts"
          VECTOR_DIR = "vector_store"
          MODEL_NAME = "all-MiniLM-L6-v2"  # A good balance between performance and speed
          BATCH_SIZE = 32  # Adjust based on your available memory

          def load_processed_transcripts():
              """Load all processed transcript chunks from JSON files"""
              print(f"Loading processed transcripts from {PROCESSED_DIR}...")
              all_chunks = []
              all_metadatas = []
              
              # Ensure directory exists
              if not os.path.exists(PROCESSED_DIR):
                  raise FileNotFoundError(f"Directory {PROCESSED_DIR} not found. Run preprocess_transcripts.py first.")
              
              # Get all JSON files
              json_files = [f for f in os.listdir(PROCESSED_DIR) if f.endswith('.json')]
              print(f"Found {len(json_files)} processed transcript files")
              
              for filename in tqdm(json_files, desc="Loading files"):
                  try:
                      file_path = os.path.join(PROCESSED_DIR, filename)
                      with open(file_path, 'r', encoding='utf-8') as f:
                          chunk_data = json.load(f)
                          
                          for chunk in chunk_data:
                              text = chunk.get('text', '')
                              metadata = chunk.get('metadata', {})
                              
                              if text and len(text.strip()) > 0:
                                  all_chunks.append(text)
                                  all_metadatas.append(metadata)
                  except Exception as e:
                      print(f"Error loading {filename}: {e}")
              
              print(f"âœ… Loaded {len(all_chunks)} transcript chunks")
              return all_chunks, all_metadatas

          def create_embeddings(texts, model_name=MODEL_NAME, batch_size=BATCH_SIZE):
              """Create embeddings for all texts using the specified model"""
              print(f"\nLoading embedding model: {model_name}")
              # Use modern SentenceTransformer without cached_download
              model = SentenceTransformer(model_name)
              
              print(f"Creating embeddings for {len(texts)} chunks (batch size: {batch_size})...")
              embeddings = []
              
              # Process in batches to avoid memory issues
              for i in tqdm(range(0, len(texts), batch_size), desc="Creating embeddings"):
                  batch_texts = texts[i:i+batch_size]
                  batch_embeddings = model.encode(batch_texts, show_progress_bar=False)
                  embeddings.extend(batch_embeddings)
              
              embeddings = np.array(embeddings).astype('float32')
              print(f"âœ… Created embeddings with shape: {embeddings.shape}")
              return embeddings

          def create_faiss_index(embeddings, metadatas, texts):
              """Create and save a FAISS index for fast similarity search"""
              # Create directory for vector store if it doesn't exist
              os.makedirs(VECTOR_DIR, exist_ok=True)
              
              # Get dimension of embeddings
              dimension = embeddings.shape[1]
              print(f"\nCreating FAISS index with dimension {dimension}...")
              
              # Create a flat index - simple but effective for smaller datasets
              index = faiss.IndexFlatL2(dimension)
              
              # Add vectors to the index
              index.add(embeddings)
              print(f"âœ… Added {index.ntotal} vectors to the index")
              
              # Save the index
              index_path = os.path.join(VECTOR_DIR, "transcript_index.faiss")
              faiss.write_index(index, index_path)
              print(f"âœ… Saved FAISS index to {index_path}")
              
              # Save the metadata mapping (needed for retrieval)
              metadata_path = os.path.join(VECTOR_DIR, "transcript_metadata.pkl")
              with open(metadata_path, 'wb') as f:
                  pickle.dump(metadatas, f)
              print(f"âœ… Saved metadata mapping to {metadata_path}")
              
              # Save raw texts for retrieval
              texts_path = os.path.join(VECTOR_DIR, "transcript_texts.pkl")
              with open(texts_path, 'wb') as f:
                  pickle.dump(texts, f)
              print(f"âœ… Saved raw texts to {texts_path}")
              
              return index

          def main(args):
              print("="*80)
              print(f"VECTOR STORE CREATION - Using model: {args.model}")
              print("="*80)
              
              # Load processed transcript chunks
              texts, metadatas = load_processed_transcripts()
              
              # Create embeddings
              embeddings = create_embeddings(texts, model_name=args.model, batch_size=args.batch_size)
              
              # Create and save FAISS index
              index = create_faiss_index(embeddings, metadatas, texts)
              
              print("\n"+"="*80)
              print("ðŸ“ Vector store creation complete!")
              print(f"âœ… Model used: {args.model}")
              print(f"âœ… Total chunks indexed: {len(texts)}")
              print(f"ðŸ“ Vector store saved to {VECTOR_DIR}/")
              print("="*80)
              print("\nTo search your vector store, use search_transcripts.py")

          if __name__ == "__main__":
              parser = argparse.ArgumentParser(description='Create a vector store from processed transcripts')
              parser.add_argument('--model', type=str, default=MODEL_NAME, 
                                  help=f'Sentence transformer model to use (default: {MODEL_NAME})')
              parser.add_argument('--batch-size', type=int, default=BATCH_SIZE,
                                  help=f'Batch size for embedding creation (default: {BATCH_SIZE})')
              parser.add_argument('--test-search', action='store_true',
                                  help='Run test queries after creating the index')
              
              args = parser.parse_args()
              main(args)
          EOF
          
          # Create app.py with modernized imports
          cat > app.py <<'EOF'
          import os
          import gradio as gr
          import pickle
          import faiss
          from sentence_transformers import SentenceTransformer
          import numpy as np
          import json
          from datetime import datetime

          # Configuration
          VECTOR_DIR = "vector_store"
          MODEL_NAME = "all-MiniLM-L6-v2"

          # Check if vector store exists
          def vector_store_exists():
              if not os.path.exists(VECTOR_DIR):
                  return False
              index_path = os.path.join(VECTOR_DIR, "transcript_index.faiss")
              metadata_path = os.path.join(VECTOR_DIR, "transcript_metadata.pkl")
              texts_path = os.path.join(VECTOR_DIR, "transcript_texts.pkl")
              return os.path.exists(index_path) and os.path.exists(metadata_path) and os.path.exists(texts_path)

          # Load vector store
          def load_vector_store():
              if not vector_store_exists():
                  raise FileNotFoundError("Vector store not found")
              
              index_path = os.path.join(VECTOR_DIR, "transcript_index.faiss")
              metadata_path = os.path.join(VECTOR_DIR, "transcript_metadata.pkl")
              texts_path = os.path.join(VECTOR_DIR, "transcript_texts.pkl")
              
              index = faiss.read_index(index_path)
              
              with open(metadata_path, 'rb') as f:
                  metadatas = pickle.load(f)
                  
              with open(texts_path, 'rb') as f:
                  texts = pickle.load(f)
                  
              return index, metadatas, texts

          # Simple search function
          def search_transcripts(query, model=None, top_k=5):
              try:
                  # Load model if not provided
                  if model is None:
                      model = SentenceTransformer(MODEL_NAME)
                      
                  # Load vector store
                  index, metadatas, texts = load_vector_store()
                  
                  # Create query embedding
                  query_embedding = model.encode([query])[0].reshape(1, -1).astype('float32')
                  
                  # Search
                  distances, indices = index.search(query_embedding, top_k)
                  
                  results = []
                  for i, (idx, distance) in enumerate(zip(indices[0], distances[0])):
                      # Get metadata for this result
                      metadata = metadatas[idx]
                      text = texts[idx]
                      
                      result = {
                          "text": text,
                          "metadata": metadata,
                          "distance": float(distance)
                      }
                      results.append(result)
                      
                  return results
              except Exception as e:
                  print(f"Error in search: {e}")
                  return []

          # Gradio interface
          def search_interface(query):
              if not query or len(query.strip()) == 0:
                  return "Please enter a search query"
              
              try:
                  model = SentenceTransformer(MODEL_NAME)
                  results = search_transcripts(query, model=model, top_k=5)
                  
                  if not results:
                      return "No results found or search error occurred"
                  
                  output = f"## Search Results for: '{query}'\n\n"
                  
                  for i, result in enumerate(results):
                      metadata = result["metadata"]
                      text = result["text"]
                      distance = result["distance"]
                      
                      title = metadata.get("title", "Untitled")
                      timestamp = metadata.get("start_timestamp", "")
                      url = metadata.get("video_url_with_timestamp", "")
                      
                      output += f"### {i+1}. {title} (at {timestamp}) - Score: {distance:.4f}\n"
                      output += f"ðŸ”— [Watch Video]({url})\n\n"
                      output += f"**Content:** {text}\n\n"
                      output += "---\n\n"
                      
                  return output
              except Exception as e:
                  return f"Error: {str(e)}"

          # Create the Gradio interface
          interface = gr.Interface(
              fn=search_interface,
              inputs=gr.Textbox(
                  lines=2, 
                  placeholder="Search for options trading concepts..."
              ),
              outputs=gr.Markdown(),
              title="Options Trading Knowledge Search",
              description="Search through hundreds of transcripts about options trading",
              theme="default"
          )

          # Simple HTML page
          def html():
              return """
              <div style="text-align: center; max-width: 800px; margin: 0 auto;">
                  <h1>Options Trading Knowledge Base</h1>
                  <p>Search through hundreds of transcripts about options trading</p>
              </div>
              """

          # Combine interfaces
          app = gr.TabbedInterface(
              [interface],
              ["Search"],
          )

          # Start the app
          if __name__ == "__main__":
              app.launch()
          EOF
          
          # Create simple app wrapper
          cat > app_wrapper.py <<'EOF'
          """
          App wrapper for Hugging Face Spaces
          """
          import sys
          import os

          # Import and run the original app
          try:
              sys.path.insert(0, os.getcwd())
              from app import app as gradio_app
              # This is needed for Gradio on HuggingFace Spaces
              app = gradio_app
          except Exception as e:
              print(f"âŒ Error importing app: {e}")
              # Create a minimal error app
              import gradio as gr
              app = gr.Interface(
                  fn=lambda x: f"Application failed to start: {str(e)}",
                  inputs=gr.Textbox(placeholder="Enter text here..."),
                  outputs=gr.Textbox()
              )
          EOF
          
          # Update the README
          cat > README.md <<EOF
          ---
          title: opteee
          emoji: ðŸ”¥
          colorFrom: blue
          colorTo: red
          sdk: gradio
          sdk_version: 4.19.2
          app_file: app_wrapper.py
          pinned: false
          ---
          
          # HuggingFace Space for opteee
          
          This space runs with updated code and modernized dependencies:
          - torch==1.13.1
          - transformers==4.26.0
          - sentence_transformers (latest compatible version)
          EOF
          
          echo "Created HuggingFace-specific configuration files:"
          ls -la

      - name: Push to Hugging Face Space
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          SPACE_NAME: "bthaile/opteee"
        run: |
          # Set credentials helper to store token
          git config --global credential.helper store
          
          # Store the Hugging Face token in the credentials helpers
          echo "https://USER:${HF_TOKEN}@huggingface.co" > ~/.git-credentials
          
          # Create a new repository with only the current content
          rm -rf .git
          git init
          git add .
          git commit -m "Deploy to Hugging Face with fixed dependencies"
          
          # Add Hugging Face Space as a Git remote
          git remote add space https://huggingface.co/spaces/$SPACE_NAME
          
          # Force push to Hugging Face Space
          git push --force space main 