name: Deploy to Hugging Face Space

on:
  push:
    branches: [ main ]
  workflow_dispatch:  # Allows manual triggering

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 1  # Only fetch the latest commit

      - name: Configure Git
        run: |
          git config --global user.email "github-actions@github.com"
          git config --global user.name "GitHub Actions"
          # Set default branch name to main
          git config --global init.defaultBranch main
          
      - name: Create Hugging Face specific files
        run: |
          # Create packages.txt for system dependencies
          echo "" > packages.txt
          
          # Create requirements.txt specifically for Hugging Face
          cat > requirements.txt <<EOF
          # Pin specific versions that work together
          torch==1.13.1
          transformers==4.26.0
          tokenizers==0.13.3
          huggingface_hub>=0.14.0
          # Vector store and other requirements
          faiss-cpu
          python-dotenv
          sentence_transformers>=2.2.2
          gradio>=3.20.0
          # Add other dependencies below but don't specify versions unless needed
          EOF
          
          # Create a modernized version of create_vector_store.py 
          cat > create_vector_store.py <<'EOF'
          import os
          import json
          import numpy as np
          from tqdm import tqdm
          import faiss
          import pickle
          from sentence_transformers import SentenceTransformer
          import argparse

          # Configuration
          PROCESSED_DIR = "processed_transcripts"
          VECTOR_DIR = "vector_store"
          MODEL_NAME = "all-MiniLM-L6-v2"  # A good balance between performance and speed
          BATCH_SIZE = 32  # Adjust based on your available memory

          def load_processed_transcripts():
              """Load all processed transcript chunks from JSON files"""
              print(f"Loading processed transcripts from {PROCESSED_DIR}...")
              all_chunks = []
              all_metadatas = []
              
              # Ensure directory exists
              if not os.path.exists(PROCESSED_DIR):
                  raise FileNotFoundError(f"Directory {PROCESSED_DIR} not found. Run preprocess_transcripts.py first.")
              
              # Get all JSON files
              json_files = [f for f in os.listdir(PROCESSED_DIR) if f.endswith('.json')]
              print(f"Found {len(json_files)} processed transcript files")
              
              for filename in tqdm(json_files, desc="Loading files"):
                  try:
                      file_path = os.path.join(PROCESSED_DIR, filename)
                      with open(file_path, 'r', encoding='utf-8') as f:
                          chunk_data = json.load(f)
                          
                          for chunk in chunk_data:
                              text = chunk.get('text', '')
                              metadata = chunk.get('metadata', {})
                              
                              if text and len(text.strip()) > 0:
                                  all_chunks.append(text)
                                  all_metadatas.append(metadata)
                  except Exception as e:
                      print(f"Error loading {filename}: {e}")
              
              print(f"âœ… Loaded {len(all_chunks)} transcript chunks")
              return all_chunks, all_metadatas

          def create_embeddings(texts, model_name=MODEL_NAME, batch_size=BATCH_SIZE):
              """Create embeddings for all texts using the specified model"""
              print(f"\nLoading embedding model: {model_name}")
              # Use modern SentenceTransformer without cached_download
              model = SentenceTransformer(model_name)
              
              print(f"Creating embeddings for {len(texts)} chunks (batch size: {batch_size})...")
              embeddings = []
              
              # Process in batches to avoid memory issues
              for i in tqdm(range(0, len(texts), batch_size), desc="Creating embeddings"):
                  batch_texts = texts[i:i+batch_size]
                  batch_embeddings = model.encode(batch_texts, show_progress_bar=False)
                  embeddings.extend(batch_embeddings)
              
              embeddings = np.array(embeddings).astype('float32')
              print(f"âœ… Created embeddings with shape: {embeddings.shape}")
              return embeddings

          def create_faiss_index(embeddings, metadatas, texts):
              """Create and save a FAISS index for fast similarity search"""
              # Create directory for vector store if it doesn't exist
              os.makedirs(VECTOR_DIR, exist_ok=True)
              
              # Get dimension of embeddings
              dimension = embeddings.shape[1]
              print(f"\nCreating FAISS index with dimension {dimension}...")
              
              # Create a flat index - simple but effective for smaller datasets
              index = faiss.IndexFlatL2(dimension)
              
              # Add vectors to the index
              index.add(embeddings)
              print(f"âœ… Added {index.ntotal} vectors to the index")
              
              # Save the index
              index_path = os.path.join(VECTOR_DIR, "transcript_index.faiss")
              faiss.write_index(index, index_path)
              print(f"âœ… Saved FAISS index to {index_path}")
              
              # Save the metadata mapping (needed for retrieval)
              metadata_path = os.path.join(VECTOR_DIR, "transcript_metadata.pkl")
              with open(metadata_path, 'wb') as f:
                  pickle.dump(metadatas, f)
              print(f"âœ… Saved metadata mapping to {metadata_path}")
              
              # Save raw texts for retrieval
              texts_path = os.path.join(VECTOR_DIR, "transcript_texts.pkl")
              with open(texts_path, 'wb') as f:
                  pickle.dump(texts, f)
              print(f"âœ… Saved raw texts to {texts_path}")
              
              return index

          def main(args):
              print("="*80)
              print(f"VECTOR STORE CREATION - Using model: {args.model}")
              print("="*80)
              
              # Load processed transcript chunks
              texts, metadatas = load_processed_transcripts()
              
              # Create embeddings
              embeddings = create_embeddings(texts, model_name=args.model, batch_size=args.batch_size)
              
              # Create and save FAISS index
              index = create_faiss_index(embeddings, metadatas, texts)
              
              print("\n"+"="*80)
              print("ðŸ“ Vector store creation complete!")
              print(f"âœ… Model used: {args.model}")
              print(f"âœ… Total chunks indexed: {len(texts)}")
              print(f"ðŸ“ Vector store saved to {VECTOR_DIR}/")
              print("="*80)
              print("\nTo search your vector store, use search_transcripts.py")

          if __name__ == "__main__":
              parser = argparse.ArgumentParser(description='Create a vector store from processed transcripts')
              parser.add_argument('--model', type=str, default=MODEL_NAME, 
                                  help=f'Sentence transformer model to use (default: {MODEL_NAME})')
              parser.add_argument('--batch-size', type=int, default=BATCH_SIZE,
                                  help=f'Batch size for embedding creation (default: {BATCH_SIZE})')
              parser.add_argument('--test-search', action='store_true',
                                  help='Run test queries after creating the index')
              
              args = parser.parse_args()
              main(args)
          EOF
          
          # Create an extremely minimal app.py without sentence_transformers
          cat > app.py <<'EOF'
          import gradio as gr
          import os
          import sys
          from datetime import datetime
          
          def minimal_search(query):
              """
              Minimal search function that doesn't use sentence_transformers
              """
              if not query or query.strip() == "":
                  return "Please enter a search query."
                  
              # Return a placeholder message
              return f"""
              # Search Results for: "{query}"
              
              This application is currently in maintenance mode. The vector search functionality 
              is being updated to work with newer dependency versions.
              
              ## What's happening?
              
              The application is being upgraded to use the latest versions of:
              - huggingface_hub
              - sentence_transformers
              - faiss-cpu
              
              ## Sample Results
              
              Here are some static example results that would normally appear:
              
              ### 1. Understanding Options Basics (at 05:23)
              ðŸ”— [Watch Video](#)
              
              **Content:** This would typically show a relevant portion of transcript about options trading basics...
              
              ---
              
              ### 2. Options Strategies for Beginners (at 12:45)
              ðŸ”— [Watch Video](#)
              
              **Content:** Information about beginner options strategies would appear here...
              
              ---
              
              ## Environment Information
              
              - Python Version: {sys.version.split()[0]}
              - Gradio Version: {gr.__version__}
              - Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
              - Available Files: {', '.join(sorted([f for f in os.listdir() if os.path.isfile(f)])[:10])}...
              """
          
          def get_info():
              """Return system information"""
              content = f"""
              # Options Trading Knowledge Base (Maintenance Mode)
              
              ## Current Status
              
              This application is currently in maintenance mode while we update dependencies.
              
              ## System Information
              
              - Python Version: {sys.version.split()[0]}
              - Gradio Version: {gr.__version__}
              - Current Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
              
              ## Installed Packages
              
              Here's a list of some installed packages:
              ```
              {os.popen('pip list | head -10').read()}
              ```
              
              ## Directory Contents
              
              Current directory contains:
              ```
              {os.popen('ls -la | head -10').read()}
              ```
              
              ## About This App
              
              This application is designed to provide semantic search across a 
              collection of options trading video transcripts. We're currently
              updating it to work with newer dependency versions.
              
              Please check back soon for the full functionality!
              """
              return content
          
          # Create search tab
          search_tab = gr.Interface(
              fn=minimal_search,
              inputs=gr.Textbox(
                  lines=2, 
                  placeholder="Search for options trading concepts..."
              ),
              outputs=gr.Markdown(),
              title="Options Trading Knowledge Search",
              description="Search through options trading transcripts (Maintenance Mode)",
              theme="default"
          )
          
          # Info tab
          info_tab = gr.Interface(
              fn=get_info,
              inputs=None,
              outputs=gr.Markdown(),
              title="System Information",
              description="Status and information about this knowledge base",
              theme="default"
          )
          
          # Combine interfaces
          app = gr.TabbedInterface(
              [search_tab, info_tab],
              ["Search", "System Info"],
          )

          # Start the app
          if __name__ == "__main__":
              app.launch()
          EOF
          
          # No need for app_wrapper.py anymore
          
          # Update the README to use app.py directly
          cat > README.md <<EOF
          ---
          title: opteee
          emoji: ðŸ”¥
          colorFrom: blue
          colorTo: red
          sdk: gradio
          sdk_version: 4.19.2
          app_file: app.py
          pinned: false
          ---
          
          # Options Trading Knowledge Search
          
          This application provides semantic search across a collection of options trading transcripts and videos.
          
          ## Features
          
          - Semantic search using sentence-transformers
          - FAISS vector database for fast retrieval
          - Direct links to specific timestamps in relevant videos
          EOF
          
          echo "Created HuggingFace-specific configuration files:"
          ls -la

      - name: Push to Hugging Face Space
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          SPACE_NAME: "bthaile/opteee"
        run: |
          # Set credentials helper to store token
          git config --global credential.helper store
          
          # Store the Hugging Face token in the credentials helpers
          echo "https://USER:${HF_TOKEN}@huggingface.co" > ~/.git-credentials
          
          # Create a new repository with only the current content
          rm -rf .git
          git init
          git add .
          git commit -m "Deploy to Hugging Face with fixed dependencies"
          
          # Add Hugging Face Space as a Git remote
          git remote add space https://huggingface.co/spaces/$SPACE_NAME
          
          # Force push to Hugging Face Space
          git push --force space main 