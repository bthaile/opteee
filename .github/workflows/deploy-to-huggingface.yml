name: Deploy to Hugging Face Space

on:
  push:
    branches: [ main ]
  workflow_dispatch:  # Allows manual triggering

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 1  # Only fetch the latest commit

      - name: Configure Git
        run: |
          git config --global user.email "github-actions@github.com"
          git config --global user.name "GitHub Actions"
          # Set default branch name to main
          git config --global init.defaultBranch main
          
      - name: Create Hugging Face specific files
        run: |
          # Create packages.txt for system dependencies
          echo "" > packages.txt
          
          # Create requirements.txt with all necessary dependencies
          cat > requirements.txt <<EOF
          # Core dependencies with pinned versions
          flask==2.0.1
          Werkzeug==2.0.1
          Jinja2==3.0.1
          itsdangerous==2.0.1
          click==8.0.1
          python-dotenv==0.21.0
          
          # Vector search dependencies
          numpy==1.23.5
          sentence-transformers==2.1.0
          faiss-cpu==1.7.3
          torch==1.13.1
          
          # Utility libraries
          markdown==3.3.7
          tqdm==4.66.1
          
          # Add Gradio for Hugging Face Spaces
          gradio==3.50.2
          EOF
          
          # Create runtime_requirements.txt with exact versions for runtime
          cat > runtime_requirements.txt <<EOF
          # Exact versions for runtime compatibility
          huggingface_hub==0.10.1
          transformers==4.20.1
          tokenizers==0.12.1
          sentence_transformers==2.1.0
          faiss-cpu==1.7.3
          EOF
          
          # Create directory structure for static files and templates
          mkdir -p static templates
          
          # Create CSS file
          cat > static/styles.css <<'EOF'
          :root {
              --bg-color: #f9f9f9;
              --container-bg: #ffffff;
              --text-color: #333333;
              --highlight-color: #4CAF50;
              --highlight-hover: #45a049;
              --border-color: #e1e1e1;
              --tab-bg: #f1f1f1;
              --tab-active: #dddddd;
              --notice-bg: #fffbea;
              --notice-border: #f0b429;
              --result-bg: #ffffff;
              --meta-color: #666666;
          }
          
          [data-theme="dark"] {
              --bg-color: #121212;
              --container-bg: #1e1e1e;
              --text-color: #e0e0e0;
              --highlight-color: #5cb85c;
              --highlight-hover: #449d44;
              --border-color: #333333;
              --tab-bg: #2a2a2a;
              --tab-active: #3a3a3a;
              --notice-bg: #2c2500;
              --notice-border: #b58603;
              --result-bg: #2a2a2a;
              --meta-color: #aaaaaa;
          }
          
          body { 
              font-family: Arial, sans-serif; 
              max-width: 800px; 
              margin: 0 auto; 
              padding: 20px;
              line-height: 1.6;
              background-color: var(--bg-color);
              color: var(--text-color);
              transition: background-color 0.3s ease, color 0.3s ease;
          }
          
          .container { 
              background-color: var(--container-bg); 
              padding: 20px; 
              border-radius: 8px;
              box-shadow: 0 2px 4px rgba(0,0,0,0.1);
          }
          
          .search-box {
              width: 100%;
              padding: 12px;
              margin: 8px 0;
              box-sizing: border-box;
              border: 2px solid var(--border-color);
              border-radius: 4px;
              font-size: 16px;
              background-color: var(--container-bg);
              color: var(--text-color);
          }
          
          .button {
              background-color: var(--highlight-color);
              color: white;
              padding: 12px 20px;
              border: none;
              border-radius: 4px;
              cursor: pointer;
              font-size: 16px;
          }
          
          .button:hover {
              background-color: var(--highlight-hover);
          }
          
          .tab {
              overflow: hidden;
              border: 1px solid var(--border-color);
              background-color: var(--tab-bg);
              border-radius: 4px 4px 0 0;
          }
          
          .tab button {
              background-color: inherit;
              float: left;
              border: none;
              outline: none;
              cursor: pointer;
              padding: 14px 16px;
              transition: 0.3s;
              font-size: 16px;
              color: var(--text-color);
          }
          
          .tab button:hover {
              background-color: var(--tab-active);
          }
          
          .tab button.active {
              background-color: var(--tab-active);
          }
          
          .tabcontent {
              display: none;
              padding: 20px;
              border: 1px solid var(--border-color);
              border-top: none;
              border-radius: 0 0 4px 4px;
              animation: fadeEffect 1s;
              background-color: var(--container-bg);
          }
          
          @keyframes fadeEffect {
              from {opacity: 0;}
              to {opacity: 1;}
          }
          
          #results {
              margin-top: 20px;
          }
          
          .result-item {
              margin-bottom: 20px;
              padding: 15px;
              border: 1px solid var(--border-color);
              border-radius: 4px;
              background-color: var(--result-bg);
          }
          
          .result-title {
              font-weight: bold;
              font-size: 18px;
              margin-bottom: 5px;
          }
          
          .result-meta {
              font-size: 14px;
              color: var(--meta-color);
              margin-bottom: 10px;
          }
          
          .result-content {
              margin-top: 10px;
          }
          
          .unavailable {
              color: var(--meta-color);
              font-style: italic;
          }
          
          .maintenance-notice {
              background-color: var(--notice-bg);
              border-left: 4px solid var(--notice-border);
              padding: 12px;
              margin-bottom: 20px;
          }
          
          .theme-toggle {
              position: absolute;
              top: 20px;
              right: 20px;
              background-color: var(--tab-bg);
              border: 1px solid var(--border-color);
              border-radius: 20px;
              padding: 5px 10px;
              cursor: pointer;
              font-size: 14px;
              color: var(--text-color);
              display: flex;
              align-items: center;
          }
          
          .theme-toggle:hover {
              background-color: var(--tab-active);
          }
          
          .theme-toggle i {
              margin-right: 5px;
          }
          
          .highlight {
              background-color: rgba(255, 193, 7, 0.3);
              padding: 0 2px;
              border-radius: 3px;
          }
          
          @media (max-width: 600px) {
              .theme-toggle {
                  position: static;
                  margin-bottom: 20px;
                  width: fit-content;
              }
          }
          
          /* Search type badges */
          .search-type-badge {
              display: inline-block;
              padding: 3px 6px;
              border-radius: 3px;
              font-size: 12px;
              margin-left: 10px;
              font-weight: normal;
          }
          
          .semantic-badge {
              background-color: var(--highlight-color);
              color: white;
          }
          
          .keyword-badge {
              background-color: #6c757d;
              color: white;
          }
          
          /* Status indicator */
          .status-indicator {
              display: inline-block;
              width: 12px;
              height: 12px;
              border-radius: 50%;
              margin-right: 5px;
          }
          
          .status-available {
              background-color: var(--highlight-color);
          }
          
          .status-building {
              background-color: #f0b429;
          }
          
          .status-unavailable {
              background-color: #dc3545;
          }
          EOF
          
          # Create JavaScript file
          cat > static/script.js <<'EOF'
          // Theme toggling functionality
          function setTheme(themeName) {
              document.documentElement.setAttribute('data-theme', themeName);
              localStorage.setItem('theme', themeName);
              
              // Update toggle button
              const themeIcon = document.getElementById('theme-icon');
              const themeText = document.getElementById('theme-text');
              
              if (themeName === 'dark') {
                  themeIcon.textContent = '‚òÄÔ∏è';
                  themeText.textContent = 'Light Mode';
              } else {
                  themeIcon.textContent = 'üåô';
                  themeText.textContent = 'Dark Mode';
              }
          }
          
          function toggleTheme() {
              const currentTheme = localStorage.getItem('theme') || 'light';
              const newTheme = currentTheme === 'light' ? 'dark' : 'light';
              setTheme(newTheme);
          }
          
          // Initialize theme from localStorage
          function initTheme() {
              const savedTheme = localStorage.getItem('theme');
              if (savedTheme) {
                  setTheme(savedTheme);
              } else {
                  // Check for system preference
                  if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
                      setTheme('dark');
                  }
              }
          }
          
          // Tab navigation
          function openTab(evt, tabName) {
              var i, tabcontent, tablinks;
              tabcontent = document.getElementsByClassName("tabcontent");
              for (i = 0; i < tabcontent.length; i++) {
                  tabcontent[i].style.display = "none";
              }
              tablinks = document.getElementsByClassName("tablinks");
              for (i = 0; i < tablinks.length; i++) {
                  tablinks[i].className = tablinks[i].className.replace(" active", "");
              }
              document.getElementById(tabName).style.display = "block";
              evt.currentTarget.className += " active";
          }
          
          // Search functionality
          function performSearch() {
              const query = document.getElementById('query').value;
              if (!query) {
                  alert('Please enter a search query');
                  return false;
              }
              
              const resultsDiv = document.getElementById('results');
              resultsDiv.innerHTML = '<p>Searching...</p>';
              
              fetch('/api/search?q=' + encodeURIComponent(query))
                  .then(response => response.json())
                  .then(data => {
                      if (data.results && data.results.length > 0) {
                          const searchTypeLabel = data.search_type === 'semantic' ? 
                              '<span style="background: #5cb85c; color: white; padding: 3px 6px; border-radius: 3px; font-size: 12px; margin-left: 10px;">SEMANTIC</span>' : 
                              '<span style="background: #6c757d; color: white; padding: 3px 6px; border-radius: 3px; font-size: 12px; margin-left: 10px;">KEYWORD</span>';
                          
                          resultsDiv.innerHTML = '<h3>Search Results for: "' + query + '"' + searchTypeLabel + '</h3>';
                          data.results.forEach(result => {
                              resultsDiv.innerHTML += generateResultHTML(result, query);
                          });
                      } else {
                          resultsDiv.innerHTML = `
                              <h3>Search Results for: "${query}"</h3>
                              <p>No matching results found. Try using different keywords.</p>
                          `;
                      }
                  })
                  .catch(error => {
                      console.error('Error:', error);
                      document.getElementById('results').innerHTML = 
                          '<p>An error occurred during search. Please try again later.</p>';
                  });
              
              return false;
          }
          
          // Highlight search terms in results
          function highlightText(text, query) {
              if (!query || query.trim() === '') return text;
              
              // Create a regex to find instances of the query terms, case-insensitive
              const terms = query.split(/\s+/).filter(term => term.length > 2);
              if (terms.length === 0) return text;
              
              const regex = new RegExp('(' + terms.join('|') + ')', 'gi');
              return text.replace(regex, '<span class="highlight">$1</span>');
          }
          
          // Generate HTML for search results
          function generateResultHTML(result, query) {
              const highlightedContent = highlightText(result.content, query);
              
              return `
                  <div class="result-item">
                      <div class="result-title">${result.title || 'Untitled'}</div>
                      <div class="result-meta">at ${result.timestamp || '00:00'} ‚Ä¢ Score: ${result.score.toFixed(4)}</div>
                      <a href="${result.video_url || '#'}" style="text-decoration:none;" target="_blank">üîó Watch Video</a>
                      <div class="result-content">
                          <strong>Content:</strong> ${highlightedContent}
                      </div>
                  </div>
              `;
          }
          
          // Initialize everything when DOM is loaded
          document.addEventListener('DOMContentLoaded', function() {
              initTheme();
              
              // Set up search form submission
              document.getElementById('search-form').addEventListener('submit', function(e) {
                  e.preventDefault();
                  performSearch();
              });
          });
          EOF
          
          # Create HTML template
          cat > templates/index.html <<'EOF'
          <!DOCTYPE html>
          <html>
          <head>
              <title>Options Trading Knowledge Search</title>
              <meta name="viewport" content="width=device-width, initial-scale=1">
              <link rel="stylesheet" href="{{ url_for('static', filename='styles.css') }}">
          </head>
          <body>
              <div class="container">
                  <button id="theme-toggle" class="theme-toggle" onclick="toggleTheme()">
                      <span id="theme-icon">üåô</span> <span id="theme-text">Dark Mode</span>
                  </button>
                  
                  <h1>Options Trading Knowledge Search</h1>
                  
                  <div class="maintenance-notice" id="status-notice">
                      <strong>üîÑ Status:</strong> 
                      {% if building_vector_store %}
                      Vector store is currently building. Basic search is available in the meantime.
                      {% else %}
                      Semantic search is {{ "available" if vector_status == "‚úÖ Available" else "not available" }}. 
                      {{ "Using keyword search as fallback." if vector_status != "‚úÖ Available" else "" }}
                      {% endif %}
                  </div>
                  
                  <div class="tab">
                      <button class="tablinks active" onclick="openTab(event, 'SearchTab')">Search</button>
                      <button class="tablinks" onclick="openTab(event, 'InfoTab')">System Info</button>
                  </div>
                  
                  <div id="SearchTab" class="tabcontent" style="display: block;">
                      <h2>Search Options Trading Knowledge</h2>
                      <form id="search-form">
                          <input type="text" id="query" class="search-box" placeholder="Search for options concepts, strategies, etc.">
                          <button type="submit" class="button">Search</button>
                      </form>
                      <div id="results"></div>
                  </div>
                  
                  <div id="InfoTab" class="tabcontent">
                      <h2>System Information</h2>
                      <h3>Current Status</h3>
                      <ul>
                          <li>Search Engine: {{ "‚úÖ Semantic search" if vector_status == "‚úÖ Available" else "üîç Keyword search" }}</li>
                          <li>Vector Store: {{ vector_status }}</li>
                          <li>Transcript Files: {{ transcript_count }}</li>
                          <li>Application Version: 2.0</li>
                          <li>Last Updated: {{ timestamp }}</li>
                      </ul>
                      
                      <h3>Environment Information</h3>
                      <ul>
                          <li>Python Version: {{ python_version }}</li>
                          <li>Web Framework: Flask {{ flask_version }}</li>
                          <li>Current Time: {{ timestamp }}</li>
                      </ul>
                      
                      <h3>Files</h3>
                      <p><strong>App Files:</strong> {{ file_list }}</p>
                      <p><strong>Static Files:</strong> {{ static_files }}</p>
                      <p><strong>Template Files:</strong> {{ template_files }}</p>
                      
                      <h3>About This App</h3>
                      <p>This application provides semantic search across a collection of options trading transcripts and videos.</p>
                      <p>The search uses embeddings from the SentenceTransformer model to find the most relevant content based on your query.</p>
                      {% if building_vector_store %}
                      <div class="maintenance-notice">
                        <strong>üöß Vector Store Building:</strong> The semantic search index is currently being built. This may take a few minutes.
                      </div>
                      {% endif %}
                  </div>
              </div>
              
              <script src="{{ url_for('static', filename='script.js') }}"></script>
              
              <script>
              // Add polling for vector store status if it's building
              {% if building_vector_store %}
              let checkStatusInterval = setInterval(function() {
                  fetch('/api/status')
                      .then(response => response.json())
                      .then(data => {
                          if (!data.building_vector_store) {
                              // Vector store build finished
                              clearInterval(checkStatusInterval);
                              // Show success message
                              const notice = document.getElementById('status-notice');
                              if (data.vector_store_available) {
                                  notice.innerHTML = '<strong>‚úÖ Ready:</strong> Semantic search is now available!';
                              } else {
                                  notice.innerHTML = '<strong>‚ùå Error:</strong> Could not build vector store. Using keyword search.';
                              }
                              // Refresh page after 2 seconds
                              setTimeout(() => location.reload(), 2000);
                          }
                      })
                      .catch(error => console.error('Error checking status:', error));
              }, 5000); // Check every 5 seconds
              {% endif %}
              </script>
          </body>
          </html>
          EOF
          
          # Create a simple gunicorn starter for HF Spaces
          cat > start.sh <<'EOF'
          #!/bin/bash
          
          # Ensure executable
          chmod +x start.sh
          
          # Print environment info
          echo "Starting application..."
          python3 --version
          echo "Current directory: $(pwd)"
          echo "Files:"
          ls -la
          
          # Run Flask app with gunicorn
          exec python3 app.py
          EOF
          
          # Make start.sh executable
          chmod +x start.sh
          
          # Update the README to specify Gradio as the SDK
          cat > README.md <<EOF
          ---
          title: opteee
          emoji: üî•
          colorFrom: blue
          colorTo: red
          sdk: gradio
          app_port: 7860
          pinned: false
          ---
          
          # Options Trading Knowledge Search
          
          This application provides semantic search across a collection of options trading transcripts and videos.
          
          ## Features
          
          - Semantic search using sentence-transformers
          - FAISS vector database for fast retrieval
          - Direct links to specific timestamps in relevant videos
          EOF
          
          # Create gradio_app.py for Hugging Face Spaces
          cat > gradio_app.py <<'EOF'
          # Gradio app wrapper for Hugging Face Spaces
          import gradio as gr
          import os
          import sys
          import threading
          from datetime import datetime
          
          print(f"===== Gradio App Starting at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} =====")
          
          # Import Flask app and start it in a background thread
          def start_flask():
              try:
                  from app import app
                  print("‚úÖ Successfully imported Flask app")
                  app.run(host="0.0.0.0", port=7860)
              except Exception as e:
                  print(f"‚ùå Error starting Flask app: {e}")
                  import traceback
                  traceback.print_exc()
          
          # Start Flask in a background thread
          flask_thread = threading.Thread(target=start_flask, daemon=True)
          flask_thread.start()
          print("‚öôÔ∏è Started Flask server in background thread")
          
          # Create a simple Gradio interface
          def dummy_function():
              return "The Options Trading Knowledge Search is running on port 7860!"
          
          # Create the Gradio interface
          demo = gr.Interface(
              fn=dummy_function,
              inputs=None,
              outputs=gr.Textbox(),
              title="Options Trading Knowledge Search",
              description="The main application is running as a Flask app on port 7860. This is just a wrapper for Hugging Face Spaces."
          )
          
          # Export the Gradio app for Hugging Face
          gradio_app = demo
          
          # Launch if running directly
          if __name__ == "__main__":
              gradio_app.launch()
          EOF
          
          # Update app.py to handle missing imports
          cat > app.py <<'EOF'
          # Flask application for Options Trading Knowledge Search with vector store building
          from flask import Flask, request, render_template, jsonify
          import os
          import sys
          import json
          from datetime import datetime
          import threading

          print(f"===== Flask App Starting at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} =====")

          # Import configuration and vector search modules
          from config import PROCESSED_DIR, VECTOR_DIR, MODEL_NAME
          from vector_search import semantic_search, vector_store_exists, build_vector_store

          # Create Flask app
          app = Flask(__name__)

          # Application status tracking
          app_status = {
              "vector_store_available": False,
              "building_vector_store": False,
              "transcript_count": 0,
              "vector_count": 0,
              "last_updated": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
          }

          def check_and_build_vector_store():
              """Check vector store status and build if needed"""
              app_status["vector_store_available"] = vector_store_exists()
              
              # If vector store doesn't exist and we're not already building it
              if not app_status["vector_store_available"] and not app_status["building_vector_store"]:
                  # Start a background thread to build the vector store
                  threading.Thread(target=build_vector_store_background).start()

          def build_vector_store_background():
              """Build vector store in a background thread"""
              try:
                  app_status["building_vector_store"] = True
                  success = build_vector_store()
                  app_status["vector_store_available"] = success
              finally:
                  app_status["building_vector_store"] = False
                  app_status["last_updated"] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

          @app.route('/')
          def home():
              """Render the home page"""
              # Check vector store status
              try:
                  check_and_build_vector_store()
              except Exception as e:
                  print(f"‚ö†Ô∏è Warning: Vector store check error: {e}")
              
              # Get system info for the Info tab
              flask_version = "2.0.1"  # Hardcoded for simplicity
              python_version = sys.version.split()[0]
              timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
              
              # Get directory listing
              app_files = sorted([f for f in os.listdir() if os.path.isfile(f)])
              static_files = []
              if os.path.exists('static'):
                  static_files = sorted(os.listdir('static'))
              template_files = []
              if os.path.exists('templates'):
                  template_files = sorted(os.listdir('templates'))
              
              # Check vector store
              vector_status = "‚úÖ Available" if vector_store_exists() else "üöß Building" if app_status["building_vector_store"] else "‚ùå Not Available"
              
              # Count transcript files
              transcript_count = 0
              if os.path.exists(PROCESSED_DIR):
                  transcript_files = [f for f in os.listdir(PROCESSED_DIR) if f.endswith('.json')]
                  transcript_count = len(transcript_files)
              
              return render_template(
                  'index.html', 
                  flask_version=flask_version,
                  python_version=python_version,
                  timestamp=timestamp,
                  file_list=', '.join(app_files[:10]) + ('...' if len(app_files) > 10 else ''),
                  static_files=', '.join(static_files[:5]) + ('...' if len(static_files) > 5 else ''),
                  template_files=', '.join(template_files[:5]) + ('...' if len(template_files) > 5 else ''),
                  vector_status=vector_status,
                  transcript_count=transcript_count,
                  building_vector_store=app_status["building_vector_store"]
              )

          @app.route('/api/search')
          def search_api():
              """Semantic search endpoint using the vector store"""
              query = request.args.get('q', '').strip()
              
              if not query:
                  return jsonify({
                      "results": [],
                      "message": "Please provide a search query"
                  })
              
              # Use semantic search
              results = semantic_search(query)
              
              # If we got results, return them
              if results:
                  return jsonify({
                      "results": results,
                      "query": query,
                      "search_type": "semantic"
                  })
              else:
                  # If no results, return empty list with explanation
                  return jsonify({
                      "results": [],
                      "query": query,
                      "search_type": "semantic",
                      "message": "No matching results found or vector store is not available."
                  })

          @app.route('/api/status')
          def status_api():
              """API endpoint for checking application status"""
              # Update status
              app_status["vector_store_available"] = vector_store_exists()
              app_status["last_updated"] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
              
              # Count transcript files
              transcript_count = 0
              if os.path.exists(PROCESSED_DIR):
                  transcript_files = [f for f in os.listdir(PROCESSED_DIR) if f.endswith('.json')]
                  transcript_count = len(transcript_files)
              app_status["transcript_count"] = transcript_count
              
              return jsonify(app_status)

          # Initialize vector store on startup
          @app.before_first_request
          def initialize_vector_store():
              """Initialize the vector store when the app starts"""
              print("Starting vector store initialization...")
              check_and_build_vector_store()

          # Run the Flask app
          if __name__ == '__main__':
              # Start building the vector store immediately
              threading.Thread(target=build_vector_store_background).start()
              print("‚öôÔ∏è Started vector store building in background thread")
              
              # Run the Flask app
              app.run(host='0.0.0.0', port=int(os.environ.get('PORT', 7860)))
          EOF
          
          # Update Dockerfile to use Gradio
          cat > Dockerfile <<'EOF'
          FROM python:3.9-slim
          
          WORKDIR /app
          
          # Copy requirements
          COPY requirements.txt .
          COPY runtime_requirements.txt .
          
          # Install dependencies
          RUN pip install --no-cache-dir -r requirements.txt
          RUN pip install --no-cache-dir -r runtime_requirements.txt
          
          # Copy application files
          COPY *.py ./
          COPY startup.sh ./
          COPY static/ ./static/
          COPY templates/ ./templates/
          COPY processed_transcripts/ ./processed_transcripts/ || true
          
          # Make directories and set permissions
          RUN mkdir -p /tmp/processed_transcripts /tmp/vector_store
          RUN chmod -R 777 /tmp
          RUN chmod +x startup.sh
          
          # Show files for debugging
          RUN ls -la
          
          EXPOSE 7860
          
          # Use startup script to build vector store and run app
          CMD ["./startup.sh"]
          EOF
          
          echo "Created HuggingFace-specific configuration files:"
          ls -la

      - name: Prepare transcript files
        run: |
          # Create a directory for processed transcripts
          mkdir -p processed_transcripts
          
          # If you have transcript files in your repo, they'll be used
          # Otherwise, add some sample files to enable testing
          if [ ! "$(ls -A processed_transcripts)" ]; then
            echo "Creating sample transcript data for testing..."
            cat > processed_transcripts/sample.json <<EOF
            [
              {
                "text": "Options give you the right, but not the obligation, to buy or sell an underlying asset at a specific price before a certain date.",
                "metadata": {
                  "title": "Options Trading Basics",
                  "start_timestamp": "05:23",
                  "start_timestamp_seconds": 323,
                  "video_id": "sample123",
                  "video_url": "https://example.com/video1",
                  "video_url_with_timestamp": "https://example.com/video1?t=323",
                  "channel_name": "Options Education"
                }
              }
            ]
            EOF
          fi
          
          echo "Transcript files prepared:"
          ls -la processed_transcripts/

      - name: Push to Hugging Face Space
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          SPACE_NAME: "bthaile/opteee"
        run: |
          # Set credentials helper to store token
          git config --global credential.helper store
          
          # Store the Hugging Face token in the credentials helpers
          echo "https://USER:${HF_TOKEN}@huggingface.co" > ~/.git-credentials
          
          # Create a new repository with only the current content
          rm -rf .git
          git init
          git add .
          git commit -m "Deploy to Hugging Face with Gradio integration"
          
          # Add Hugging Face Space as a Git remote
          git remote add space https://huggingface.co/spaces/$SPACE_NAME
          
          # Force push to Hugging Face Space
          git push --force space main 

      - name: Create vector_search.py
        run: |
          cat > vector_search.py <<'EOF'
          import os
          import json
          import pickle
          import numpy as np
          from sentence_transformers import SentenceTransformer
          import faiss
          from tqdm import tqdm
          import time
          import sys
          
          # Configuration
          PROCESSED_DIR = "/tmp/processed_transcripts"
          VECTOR_DIR = "/tmp/vector_store"
          MODEL_NAME = "all-MiniLM-L6-v2"
          BATCH_SIZE = 32
          
          # Global model instance (loaded when needed)
          _model = None
          
          def get_model():
              """Get or initialize the embedding model"""
              global _model
              if _model is None:
                  print(f"Loading model: {MODEL_NAME}")
                  _model = SentenceTransformer(MODEL_NAME)
              return _model
              
          def vector_store_exists():
              """Check if vector store files exist"""
              index_path = os.path.join(VECTOR_DIR, "transcript_index.faiss")
              metadata_path = os.path.join(VECTOR_DIR, "transcript_metadata.pkl")
              texts_path = os.path.join(VECTOR_DIR, "transcript_texts.pkl")
              return all(os.path.exists(f) for f in [index_path, metadata_path, texts_path])
              
          def load_processed_transcripts():
              """Load all processed transcript chunks from JSON files"""
              print(f"Loading processed transcripts from {PROCESSED_DIR}...")
              all_chunks = []
              all_metadatas = []
              
              # Check if directory exists
              if not os.path.exists(PROCESSED_DIR):
                  print(f"Directory {PROCESSED_DIR} not found")
                  return [], []
              
              # Get all JSON files
              json_files = [f for f in os.listdir(PROCESSED_DIR) if f.endswith('.json')]
              if not json_files:
                  print(f"No JSON files found in {PROCESSED_DIR}")
                  return [], []
                  
              print(f"Found {len(json_files)} processed transcript files")
              
              # Use a progress bar for loading
              for filename in tqdm(json_files, desc="Loading files"):
                  try:
                      file_path = os.path.join(PROCESSED_DIR, filename)
                      with open(file_path, 'r', encoding='utf-8') as f:
                          chunk_data = json.load(f)
                          
                          for chunk in chunk_data:
                              text = chunk.get('text', '')
                              metadata = chunk.get('metadata', {})
                              
                              if text and len(text.strip()) > 0:
                                  all_chunks.append(text)
                                  all_metadatas.append(metadata)
                  except Exception as e:
                      print(f"Error loading {filename}: {e}")
              
              print(f"‚úÖ Loaded {len(all_chunks)} transcript chunks")
              return all_chunks, all_metadatas
              
          def create_embeddings(texts):
              """Create embeddings for all texts using the specified model"""
              model = get_model()
              embeddings = []
              
              # Process in batches to avoid memory issues
              print(f"Creating embeddings for {len(texts)} chunks (batch size: {BATCH_SIZE})...")
              
              for i in tqdm(range(0, len(texts), BATCH_SIZE), desc="Creating embeddings"):
                  batch_texts = texts[i:i+BATCH_SIZE]
                  batch_embeddings = model.encode(batch_texts, show_progress_bar=False)
                  embeddings.extend(batch_embeddings)
              
              embeddings = np.array(embeddings).astype('float32')
              print(f"‚úÖ Created embeddings with shape: {embeddings.shape}")
              return embeddings
              
          def create_faiss_index(embeddings, metadatas, texts):
              """Create and save a FAISS index for fast similarity search"""
              # Create directory for vector store if it doesn't exist
              os.makedirs(VECTOR_DIR, exist_ok=True)
              
              # Get dimension of embeddings
              dimension = embeddings.shape[1]
              print(f"Creating FAISS index with dimension {dimension}...")
              
              # Create a flat index - simple but effective for smaller datasets
              index = faiss.IndexFlatL2(dimension)
              
              # Add vectors to the index
              index.add(embeddings)
              print(f"‚úÖ Added {index.ntotal} vectors to the index")
              
              # Save the index
              index_path = os.path.join(VECTOR_DIR, "transcript_index.faiss")
              faiss.write_index(index, index_path)
              print(f"‚úÖ Saved FAISS index to {index_path}")
              
              # Save the metadata mapping (needed for retrieval)
              metadata_path = os.path.join(VECTOR_DIR, "transcript_metadata.pkl")
              with open(metadata_path, 'wb') as f:
                  pickle.dump(metadatas, f)
              print(f"‚úÖ Saved metadata mapping to {metadata_path}")
              
              # Save raw texts for retrieval
              texts_path = os.path.join(VECTOR_DIR, "transcript_texts.pkl")
              with open(texts_path, 'wb') as f:
                  pickle.dump(texts, f)
              print(f"‚úÖ Saved raw texts to {texts_path}")
              
              return index
              
          def build_vector_store():
              """Build or rebuild the vector store from processed transcripts"""
              start_time = time.time()
              
              print("\n" + "="*50)
              print(f"BUILDING VECTOR STORE - Using model: {MODEL_NAME}")
              print("="*50)
              
              # Load transcripts
              texts, metadatas = load_processed_transcripts()
              
              if not texts:
                  print("‚ùå No transcript chunks loaded, cannot build vector store")
                  return False
                  
              try:
                  # Create embeddings
                  embeddings = create_embeddings(texts)
                  
                  # Create index
                  create_faiss_index(embeddings, metadatas, texts)
                  
                  elapsed = time.time() - start_time
                  print(f"\n‚úÖ Vector store built successfully in {elapsed:.1f} seconds")
                  return True
              except Exception as e:
                  print(f"‚ùå Error building vector store: {e}")
                  import traceback
                  traceback.print_exc()
                  return False
                  
          def load_vector_store():
              """Load the vector store from disk"""
              if not vector_store_exists():
                  print("‚ö†Ô∏è Vector store does not exist")
                  return None, [], []
                  
              try:
                  index_path = os.path.join(VECTOR_DIR, "transcript_index.faiss")
                  metadata_path = os.path.join(VECTOR_DIR, "transcript_metadata.pkl")
                  texts_path = os.path.join(VECTOR_DIR, "transcript_texts.pkl")
                  
                  # Load FAISS index
                  index = faiss.read_index(index_path)
                  
                  # Load metadata and texts
                  with open(metadata_path, 'rb') as f:
                      metadatas = pickle.load(f)
                      
                  with open(texts_path, 'rb') as f:
                      texts = pickle.load(f)
                      
                  print(f"‚úÖ Loaded vector store with {index.ntotal} vectors")
                  return index, metadatas, texts
              except Exception as e:
                  print(f"‚ùå Error loading vector store: {e}")
                  return None, [], []
                  
          def semantic_search(query, top_k=5):
              """Perform semantic search using the vector store"""
              # Check if vector store exists and try to build it if not
              if not vector_store_exists():
                  print("Vector store doesn't exist, attempting to build...")
                  success = build_vector_store()
                  if not success:
                      return []
              
              # Load vector store
              index, metadatas, texts = load_vector_store()
              if index is None:
                  return []
                  
              # Get model and create query embedding
              model = get_model()
              query_embedding = model.encode([query])[0].reshape(1, -1).astype('float32')
              
              # Search
              distances, indices = index.search(query_embedding, top_k)
              
              results = []
              for i, (idx, distance) in enumerate(zip(indices[0], distances[0])):
                  if idx >= len(texts) or idx < 0:
                      continue
                      
                  metadata = metadatas[idx]
                  text = texts[idx]
                  
                  # Normalize distance to a score between 0 and 1
                  # Lower distance is better in L2 space, so we invert it
                  score = 1.0 / (1.0 + distance)
                  
                  results.append({
                      "title": metadata.get("title", f"Result {i+1}"),
                      "timestamp": metadata.get("start_timestamp", ""),
                      "video_url": metadata.get("video_url_with_timestamp", "#"),
                      "content": text,
                      "score": score
                  })
                  
              return results
          EOF
          echo "‚úÖ Created vector_search.py"

      - name: Create startup script
        run: |
          cat > startup.sh <<'EOF'
          #!/bin/bash
          
          echo "===== Starting application setup ====="
          date
          echo "Current directory: $(pwd)"
          
          # Check for transcript files
          echo "Checking for transcript files..."
          if [ -d "processed_transcripts" ]; then
            echo "‚úÖ Found processed_transcripts directory"
            mkdir -p /tmp/processed_transcripts
            cp -r processed_transcripts/* /tmp/processed_transcripts/ || echo "Error copying files"
            echo "Copied transcripts to /tmp/processed_transcripts:"
            ls -la /tmp/processed_transcripts
          else
            echo "‚ùå No processed_transcripts directory found"
          fi
          
          # Set permissions
          chmod -R 777 /tmp
          
          # Build the vector store
          echo "===== Building vector store ====="
          python -c "from vector_search import build_vector_store; build_vector_store()"
          
          # Check if vector store was built
          if [ -d "/tmp/vector_store" ]; then
            echo "‚úÖ Vector store directory exists:"
            ls -la /tmp/vector_store
          else
            echo "‚ùå Vector store directory does not exist"
          fi
          
          # Start the Flask app
          echo "===== Starting Flask app ====="
          python app.py
          EOF
          
          chmod +x startup.sh
          echo "‚úÖ Created startup.sh"

      - name: Create config.py
        run: |
          cat > config.py <<'EOF'
          import os
          
          # Directories - use /tmp for writable storage
          PROCESSED_DIR = "/tmp/processed_transcripts"
          VECTOR_DIR = "/tmp/vector_store"
          
          # Model configuration
          MODEL_NAME = "all-MiniLM-L6-v2"
          
          # Search configuration
          DEFAULT_TOP_K = 5
          
          # Processing configuration
          BATCH_SIZE = 32
          
          # Ensure directories exist
          os.makedirs(PROCESSED_DIR, exist_ok=True)
          os.makedirs(VECTOR_DIR, exist_ok=True)
          EOF
          echo "‚úÖ Created config.py" 