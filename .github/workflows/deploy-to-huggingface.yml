name: Deploy to Hugging Face Space

on:
  push:
    branches: [ main ]
  workflow_dispatch:  # Allows manual triggering

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 1  # Only fetch the latest commit

      - name: Configure Git
        run: |
          git config --global user.email "github-actions@github.com"
          git config --global user.name "GitHub Actions"
          # Set default branch name to main
          git config --global init.defaultBranch main
          
      - name: Create Hugging Face specific files
        run: |
          # Create packages.txt for system dependencies
          echo "" > packages.txt
          
          # Create requirements.txt specifically for Hugging Face
          cat > requirements.txt <<EOF
          # Pin specific versions that work together
          torch==1.13.1
          transformers==4.26.0
          tokenizers==0.13.3
          huggingface_hub>=0.14.0
          # Vector store and other requirements
          faiss-cpu
          python-dotenv
          sentence_transformers>=2.2.2
          gradio>=3.20.0
          # Add other dependencies below but don't specify versions unless needed
          EOF
          
          # Create a modernized version of create_vector_store.py 
          cat > create_vector_store.py <<'EOF'
          import os
          import json
          import numpy as np
          from tqdm import tqdm
          import faiss
          import pickle
          from sentence_transformers import SentenceTransformer
          import argparse

          # Configuration
          PROCESSED_DIR = "processed_transcripts"
          VECTOR_DIR = "vector_store"
          MODEL_NAME = "all-MiniLM-L6-v2"  # A good balance between performance and speed
          BATCH_SIZE = 32  # Adjust based on your available memory

          def load_processed_transcripts():
              """Load all processed transcript chunks from JSON files"""
              print(f"Loading processed transcripts from {PROCESSED_DIR}...")
              all_chunks = []
              all_metadatas = []
              
              # Ensure directory exists
              if not os.path.exists(PROCESSED_DIR):
                  raise FileNotFoundError(f"Directory {PROCESSED_DIR} not found. Run preprocess_transcripts.py first.")
              
              # Get all JSON files
              json_files = [f for f in os.listdir(PROCESSED_DIR) if f.endswith('.json')]
              print(f"Found {len(json_files)} processed transcript files")
              
              for filename in tqdm(json_files, desc="Loading files"):
                  try:
                      file_path = os.path.join(PROCESSED_DIR, filename)
                      with open(file_path, 'r', encoding='utf-8') as f:
                          chunk_data = json.load(f)
                          
                          for chunk in chunk_data:
                              text = chunk.get('text', '')
                              metadata = chunk.get('metadata', {})
                              
                              if text and len(text.strip()) > 0:
                                  all_chunks.append(text)
                                  all_metadatas.append(metadata)
                  except Exception as e:
                      print(f"Error loading {filename}: {e}")
              
              print(f"âœ… Loaded {len(all_chunks)} transcript chunks")
              return all_chunks, all_metadatas

          def create_embeddings(texts, model_name=MODEL_NAME, batch_size=BATCH_SIZE):
              """Create embeddings for all texts using the specified model"""
              print(f"\nLoading embedding model: {model_name}")
              # Use modern SentenceTransformer without cached_download
              model = SentenceTransformer(model_name)
              
              print(f"Creating embeddings for {len(texts)} chunks (batch size: {batch_size})...")
              embeddings = []
              
              # Process in batches to avoid memory issues
              for i in tqdm(range(0, len(texts), batch_size), desc="Creating embeddings"):
                  batch_texts = texts[i:i+batch_size]
                  batch_embeddings = model.encode(batch_texts, show_progress_bar=False)
                  embeddings.extend(batch_embeddings)
              
              embeddings = np.array(embeddings).astype('float32')
              print(f"âœ… Created embeddings with shape: {embeddings.shape}")
              return embeddings

          def create_faiss_index(embeddings, metadatas, texts):
              """Create and save a FAISS index for fast similarity search"""
              # Create directory for vector store if it doesn't exist
              os.makedirs(VECTOR_DIR, exist_ok=True)
              
              # Get dimension of embeddings
              dimension = embeddings.shape[1]
              print(f"\nCreating FAISS index with dimension {dimension}...")
              
              # Create a flat index - simple but effective for smaller datasets
              index = faiss.IndexFlatL2(dimension)
              
              # Add vectors to the index
              index.add(embeddings)
              print(f"âœ… Added {index.ntotal} vectors to the index")
              
              # Save the index
              index_path = os.path.join(VECTOR_DIR, "transcript_index.faiss")
              faiss.write_index(index, index_path)
              print(f"âœ… Saved FAISS index to {index_path}")
              
              # Save the metadata mapping (needed for retrieval)
              metadata_path = os.path.join(VECTOR_DIR, "transcript_metadata.pkl")
              with open(metadata_path, 'wb') as f:
                  pickle.dump(metadatas, f)
              print(f"âœ… Saved metadata mapping to {metadata_path}")
              
              # Save raw texts for retrieval
              texts_path = os.path.join(VECTOR_DIR, "transcript_texts.pkl")
              with open(texts_path, 'wb') as f:
                  pickle.dump(texts, f)
              print(f"âœ… Saved raw texts to {texts_path}")
              
              return index

          def main(args):
              print("="*80)
              print(f"VECTOR STORE CREATION - Using model: {args.model}")
              print("="*80)
              
              # Load processed transcript chunks
              texts, metadatas = load_processed_transcripts()
              
              # Create embeddings
              embeddings = create_embeddings(texts, model_name=args.model, batch_size=args.batch_size)
              
              # Create and save FAISS index
              index = create_faiss_index(embeddings, metadatas, texts)
              
              print("\n"+"="*80)
              print("ðŸ“ Vector store creation complete!")
              print(f"âœ… Model used: {args.model}")
              print(f"âœ… Total chunks indexed: {len(texts)}")
              print(f"ðŸ“ Vector store saved to {VECTOR_DIR}/")
              print("="*80)
              print("\nTo search your vector store, use search_transcripts.py")

          if __name__ == "__main__":
              parser = argparse.ArgumentParser(description='Create a vector store from processed transcripts')
              parser.add_argument('--model', type=str, default=MODEL_NAME, 
                                  help=f'Sentence transformer model to use (default: {MODEL_NAME})')
              parser.add_argument('--batch-size', type=int, default=BATCH_SIZE,
                                  help=f'Batch size for embedding creation (default: {BATCH_SIZE})')
              parser.add_argument('--test-search', action='store_true',
                                  help='Run test queries after creating the index')
              
              args = parser.parse_args()
              main(args)
          EOF
          
          # Create a fallback app that doesn't import anything from the project
          cat > app.py <<'EOF'
          import gradio as gr
          import os

          # Simple fallback app that doesn't rely on any existing code
          def search_placeholder(query):
              return f"""
          # Under Maintenance
          
          This application is currently being upgraded to work with newer dependencies.
          
          **Your search query was:** {query}
          
          The full search functionality will be restored soon. Please check back later.
          
          ## Environment Information
          
          - Python modules: {', '.join(sorted(list(sys.modules.keys())[:20]))}...
          - Gradio version: {gr.__version__}
          - Current directory files: {', '.join(os.listdir('.')[:20])}...
          """

          # Create a very minimal app
          app = gr.Interface(
              fn=search_placeholder,
              inputs=gr.Textbox(
                  lines=2, 
                  placeholder="Search functionality currently under maintenance..."
              ),
              outputs=gr.Markdown(),
              title="Options Trading Knowledge Search (Maintenance Mode)",
              description="This application is currently being upgraded. Full search functionality will be restored soon.",
              theme="default"
          )

          # Start the app
          if __name__ == "__main__":
              app.launch()
          EOF
          
          # Create a direct app launcher that doesn't use app_wrapper.py
          cat > README.md <<EOF
          ---
          title: opteee
          emoji: ðŸ”¥
          colorFrom: blue
          colorTo: red
          sdk: gradio
          sdk_version: 4.19.2
          app_file: app.py
          pinned: false
          ---
          
          # HuggingFace Space for opteee (Maintenance Mode)
          
          This space is currently under maintenance as we upgrade dependencies.
          
          Normal service will resume shortly.
          EOF
          
          echo "Created HuggingFace-specific configuration files:"
          ls -la

      - name: Push to Hugging Face Space
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          SPACE_NAME: "bthaile/opteee"
        run: |
          # Set credentials helper to store token
          git config --global credential.helper store
          
          # Store the Hugging Face token in the credentials helpers
          echo "https://USER:${HF_TOKEN}@huggingface.co" > ~/.git-credentials
          
          # Create a new repository with only the current content
          rm -rf .git
          git init
          git add .
          git commit -m "Deploy to Hugging Face with fixed dependencies"
          
          # Add Hugging Face Space as a Git remote
          git remote add space https://huggingface.co/spaces/$SPACE_NAME
          
          # Force push to Hugging Face Space
          git push --force space main 