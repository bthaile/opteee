---
title: opteee
emoji: üî•
colorFrom: blue
colorTo: red
sdk: docker
app_port: 7860
pinned: false
---

# OPTEEE - Options Trading Knowledge Search

AI-powered search through Outlier Trading educational videos using RAG (Retrieval-Augmented Generation).

##  Quick Start

**Every new terminal session requires these commands:**

```bash
cd /Users/bthaile/gitrepos/opteee
source venv/bin/activate

# Run the main application
python3 app.py
# Visit: http://localhost:7860
```

## ü§ñ **Automated Weekly Updates**

The system automatically stays current with new Outlier Trading videos through a **GitHub Actions workflow** that runs every Sunday at 8:00 PM UTC (3:00 PM CT).

### **üîÑ Automatic Processing Workflow**

**Schedule:** Every Sunday at 8:00 PM UTC (3:00 PM CT - avoiding DST issues)

**What it does:**
1. ** Video Discovery** - Scans for new Outlier Trading videos
2. **üé§ Transcript Generation** - Creates transcripts using OpenAI Whisper
3. ** Text Processing** - Chunks transcripts into searchable segments
4. **üì§ Deployment** - Commits processed files and triggers Hugging Face deployment
5. **üîÆ Vector Store Creation** - Happens automatically on Hugging Face during Docker build

**Files automatically updated:**
- `outlier_trading_videos.json` - Video metadata
- `transcripts/` - Raw transcript files (timestamped)
- `processed_transcripts/` - Chunked transcript data

**Files created on Hugging Face:**
- `vector_store/` - FAISS vector database (built during Docker image creation)

### **üéØ Smart Processing Logic**

The workflow uses **intelligent processing** to avoid unnecessary work:

- **New videos only**: Only processes videos that don't already have transcripts
- **Incremental updates**: Only rebuilds vector store when new content is added
- **Change detection**: Only commits and deploys when new content is found
- **Non-interactive mode**: Runs completely automated without user input

### **üìä Processing Reports**

Each workflow run generates:
- **üìã Processing Summary** - Videos discovered, transcripts generated, chunks created
- **üîÑ GitHub Commit** - Detailed commit message with processing statistics
- **üìà Workflow Summary** - GitHub Actions summary with results
- ** Auto-deployment** - Triggers Hugging Face Space update

### **üîß Workflow Configuration**

**Location:** `.github/workflows/process-transcripts.yml`

**Environment Variables Required:**
- `YOUTUBE_API_KEY` - For enhanced video metadata
- `GITHUB_TOKEN` - Automatically provided by GitHub

**Performance Settings:**
- **Timeout:** 3 hours for large processing jobs
- **Processing Mode:** Non-interactive (CI/CD optimized)
- **Chunk Configuration:** 250 words/chunk, 50 words overlap
- **Architecture:** GitHub Actions handles steps 1-3, Hugging Face handles step 4

### **üì± Manual Triggering**

You can manually trigger the workflow:

1. **Via GitHub UI:**
   - Go to Actions tab in the repository
   - Select "Process Video Transcripts Weekly"
   - Click "Run workflow"

2. **Via GitHub CLI:**
   ```bash
   gh workflow run process-transcripts.yml
   ```

### ** Monitoring**

**Check workflow status:**
- GitHub Actions tab shows all runs
- Each run provides detailed logs
- Workflow summaries show processing results
- Commit messages include processing statistics

**Verify updates:**
- New transcripts appear in `transcripts/` directory
- Processed files appear in `processed_transcripts/` directory
- Hugging Face Space automatically rebuilds and creates vector store
- Vector store files appear in `vector_store/` directory on Hugging Face

### **üéâ Benefits**

- **üîÑ Always Current**: Never miss new Outlier Trading videos
- **‚ö° Fast Processing**: Optimized pipeline with parallel processing
- **üìä Transparent**: Detailed reporting on what was processed
- ** Zero Maintenance**: Fully automated - no manual intervention needed
- **üíé Smart**: Only processes new content, avoiding redundant work

## üß™ **Testing the New Pipeline**

### **üî¨ Local Testing**

**1. Test Complete Pipeline (Interactive Mode)**
```bash
# Test the enhanced pipeline with all steps
python3 run_pipeline.py

# Test with force reprocessing
python3 run_pipeline.py --force-reprocess

# Test individual steps
python3 run_pipeline.py --step scrape
python3 run_pipeline.py --step transcripts
python3 run_pipeline.py --step preprocess
python3 run_pipeline.py --step vectors
```

**2. Test Non-Interactive Mode (CI/CD Simulation)**
```bash
# Test the exact mode used by GitHub Actions (steps 1-3 only)
python3 run_pipeline.py --step scrape --non-interactive
python3 run_pipeline.py --step transcripts --non-interactive
python3 run_pipeline.py --step preprocess --non-interactive

# Test with force reprocessing in non-interactive mode
python3 run_pipeline.py --step transcripts --non-interactive --force-reprocess

# Test the complete pipeline including vector store (for local testing)
python3 run_pipeline.py --non-interactive
```

**3. Test Smart Processing Logic**
```bash
# First run - should process everything (steps 1-3)
python3 run_pipeline.py --step scrape --non-interactive
python3 run_pipeline.py --step transcripts --non-interactive
python3 run_pipeline.py --step preprocess --non-interactive

# Second run - should detect no changes and skip processing
python3 run_pipeline.py --step transcripts --non-interactive
python3 run_pipeline.py --step preprocess --non-interactive

# Test complete pipeline locally (including vector store)
python3 run_pipeline.py --non-interactive
```

**4. Validate Pipeline Results**
```bash
# Run comprehensive system validation
python3 validate_system.py

# Test RAG pipeline with sample questions
python3 rag_pipeline.py "What is gamma in options trading?" --provider claude

# Check file counts and structure
python3 count_files.py transcripts
python3 count_files.py processed_transcripts
ls -la vector_store/
```

### **ü§ñ GitHub Actions Testing**

**1. Test Workflow Manually**
```bash
# Using GitHub CLI
gh workflow run process-transcripts.yml

# Or via GitHub web interface:
# 1. Go to your repository on GitHub
# 2. Click "Actions" tab
# 3. Select "Process Video Transcripts Weekly"
# 4. Click "Run workflow" button
```

**2. Test Workflow with Different Scenarios**

**Scenario A: Fresh Repository (First Run)**
```bash
# Remove existing files to simulate fresh start
rm -rf transcripts/ processed_transcripts/
rm -f outlier_trading_videos*.json

# Push changes and trigger workflow
git add -A && git commit -m "Test: Fresh repository state"
git push
gh workflow run process-transcripts.yml
```

**Scenario B: Incremental Update (New Videos)**
```bash
# Simulate new videos by removing some transcripts
rm transcripts/[some_video_id].txt

# Push changes and trigger workflow
git add -A && git commit -m "Test: Simulate new videos"
git push
gh workflow run process-transcripts.yml
```

**Scenario C: No Changes (Skip Processing)**
```bash
# Run workflow when everything is up to date
gh workflow run process-transcripts.yml
# Should detect no changes and skip processing
```

**3. Monitor Workflow Execution**
```bash
# Watch workflow in real-time
gh run watch

# List recent workflow runs
gh run list --workflow=process-transcripts.yml

# View specific run details
gh run view [RUN_ID]

# Download workflow logs
gh run download [RUN_ID]
```

### ** Verification Steps**

**1. Check Workflow Results**
```bash
# After workflow completes, verify files were created/updated
git pull  # Get latest changes from GitHub

# Check video discovery
python3 -c "
import json
with open('outlier_trading_videos.json') as f:
    data = json.load(f)
print(f'‚úÖ Videos discovered: {len(data)}')
"

# Check transcripts
find transcripts -name "*.txt" | wc -l

# Check processed transcripts
find processed_transcripts -name "*.json" | wc -l

# Note: Vector store creation happens on Hugging Face during Docker build
echo "Vector store will be created on Hugging Face"
```

**2. Verify GitHub Actions Outputs**
- Check the "Actions" tab for workflow status
- Review the workflow summary (should show processing results)
- Check commit messages for processing statistics
- Verify Hugging Face deployment was triggered

**3. Test End-to-End Functionality**
```bash
# Test the complete system after workflow updates
python3 app.py
# Visit http://localhost:7860 and test search functionality
```

### **üêõ Debugging Failed Tests**

**1. Local Pipeline Issues**
```bash
# Run GitHub Actions steps locally for debugging
python3 run_pipeline.py --step scrape --non-interactive 2>&1 | tee scrape_debug.log
python3 run_pipeline.py --step transcripts --non-interactive 2>&1 | tee transcripts_debug.log
python3 run_pipeline.py --step preprocess --non-interactive 2>&1 | tee preprocess_debug.log

# Check specific component issues
python3 validate_system.py
python3 check_audio_files.py
```

**2. GitHub Actions Issues**
```bash
# Check workflow logs
gh run view [RUN_ID] --log

# Check repository secrets
gh secret list

# Test API keys locally
python3 -c "
import os
from dotenv import load_dotenv
load_dotenv()
print(f'YouTube API Key: {bool(os.getenv(\"YOUTUBE_API_KEY\"))}')
# Note: OpenAI/Anthropic API keys only needed for RAG functionality (not in GitHub Actions)
print(f'OpenAI API Key: {bool(os.getenv(\"OPENAI_API_KEY\"))}')
print(f'Anthropic API Key: {bool(os.getenv(\"ANTHROPIC_API_KEY\"))}')
"
```

**3. Hugging Face Deployment Issues**
```bash
# Check deployment webhook
gh run view [RUN_ID] --log | grep "repository-dispatch"

# Verify Hugging Face token
gh secret list | grep HF_TOKEN
```

### **‚ö° Quick Testing Commands**

**Test Basic Functionality:**
```bash
# Quick GitHub Actions simulation (should complete in 5-10 minutes for testing)
python3 run_pipeline.py --step scrape --non-interactive
python3 run_pipeline.py --step transcripts --non-interactive
python3 run_pipeline.py --step preprocess --non-interactive
python3 validate_system.py
```

**Test GitHub Actions Locally:**
```bash
# Install act (GitHub Actions local runner)
brew install act  # macOS
# OR
curl https://raw.githubusercontent.com/nektos/act/master/install.sh | sudo bash

# Run GitHub Actions workflow locally
act workflow_dispatch -W .github/workflows/process-transcripts.yml
```

**Test Performance:**
```bash
# Time the complete pipeline
time python3 run_pipeline.py --non-interactive

# Check memory usage
/usr/bin/time -v python3 run_pipeline.py --non-interactive
```

### **üìä Expected Test Results**

**‚úÖ Successful Pipeline Run Should Show:**
- Video discovery: ~500+ videos found
- Transcript generation: New transcripts created for missing videos
- Processing: ~14,000+ chunks created
- Vector store: Created on Hugging Face during Docker build
- No errors in validation steps

**‚úÖ Successful GitHub Actions Run Should Show:**
- Workflow completes within 3 hours
- All verification steps pass (video discovery, transcripts, preprocessing)
- Changes committed to repository (if new content found)
- Hugging Face deployment triggered (creates vector store during Docker build)
- Detailed processing summary in workflow logs

**‚úÖ Successful Non-Interactive Mode Should Show:**
- No user prompts or input requests
- Intelligent skipping of up-to-date content
- Proper error handling and logging
- Clean exit codes (0 for success)

## üìã **Complete Processing Pipeline with Verification**

### **Step 1: Video Discovery & Metadata Collection**

**Run:**
```bash
python3 run_pipeline.py --step scrape
```

**Verification:**
```bash
# Check video metadata files exist
ls -la outlier_trading_videos*.json

# Verify video count
python3 -c "
import json
with open('outlier_trading_videos.json') as f:
    data = json.load(f)
print(f'‚úÖ Discovered {len(data)} videos')
"
```

**Expected Output:**
- `outlier_trading_videos.json` - Basic video metadata
- `outlier_trading_videos_metadata.json` - Enhanced metadata (if YouTube API key available)
- ~500+ videos discovered

### **Step 2: Transcript Generation**

**Standard Processing:**
```bash
python3 run_pipeline.py --step transcripts
```

**‚ö° Parallel Processing (Recommended for M1 Max/High-End Systems):**
```bash
# Use default parallel settings (8 workers, "base" model)
python3 parallel_transcribe.py

# Custom parallel configuration
python3 parallel_transcribe.py --workers 8 --model base --gpu

# Fix existing timestamps with parallel processing
python3 fix_timestamp_issue.py --parallel --workers 8
```

**Verification:**
```bash
# Check transcript files
ls transcripts/ | wc -l

# Verify transcript format has timestamps
head -5 transcripts/[VIDEO_ID].txt

# Check parallel processing progress
cat transcripts/parallel_progress.json
```

**Expected Output:**
- Files in `transcripts/` directory with format `[VIDEO_ID].txt`
- Each transcript line formatted as: `XX.XXs: transcript content`
- Progress tracking in `transcript_progress.json` or `parallel_progress.json`
- **Parallel processing**: 3-10x faster than sequential processing

### **Step 3: Transcript Processing & Chunking**

**Run:**
```bash
python3 run_pipeline.py --step preprocess
```

**Verification:**
```bash
# Check processed transcript chunks
ls processed_transcripts/ | wc -l

# Verify chunk structure
python3 -c "
import json
import glob
files = glob.glob('processed_transcripts/*.json')
if files:
    with open(files[0]) as f:
        chunk = json.load(f)
    print(f'‚úÖ Sample chunk has {len(chunk)} fields')
    print(f'‚úÖ Required fields: {\"start_timestamp_seconds\" in chunk}')
    print(f'‚úÖ Video URL: {chunk.get(\"video_url_with_timestamp\", \"Missing\")}')
"
```

**Expected Output:**
- JSON files in `processed_transcripts/` directory
- Each chunk contains: `video_id`, `title`, `start_timestamp_seconds`, `video_url_with_timestamp`
- ~14,000+ chunks created

### **Step 4: Vector Store Creation**

**Run:**
```bash
python3 run_pipeline.py --step vectors
```

**Verification:**
```bash
# Check vector store files
ls -la vector_store/

# Verify vector store contents
python3 -c "
import pickle
with open('vector_store/transcript_metadata.pkl', 'rb') as f:
    metadata = pickle.load(f)
print(f'‚úÖ Vector store contains {len(metadata)} chunks')
timestamps = sum(1 for chunk in metadata if chunk.get('start_timestamp_seconds', 0) > 0)
print(f'‚úÖ Chunks with timestamps: {timestamps}')
"
```

**Expected Output:**
- `vector_store/transcript_embeddings.faiss` - FAISS index (~60MB)
- `vector_store/transcript_metadata.pkl` - Chunk metadata
- All chunks should have proper timestamps (not 0)

### **Step 5: System Validation**

**Run:**
```bash
python3 validate_system.py
```

**Verification:**
```bash
# Test the RAG pipeline
python3 rag_pipeline.py "What is gamma in options trading?" --provider claude

# Check for working video links with timestamps
python3 -c "
import re
# This should show video links with &t=XXX timestamps
"
```

**Expected Output:**
- All API keys validated
- Vector store verified
- RAG pipeline returns answers with timestamped video links

## ‚ö° **Parallel Processing & Performance Optimization**

### ** High-Performance Transcription**

For systems with multiple CPU cores (especially M1 Max, M2, or high-end Intel/AMD), parallel processing can dramatically speed up transcription:

```bash
# Quick system analysis
python3 parallel_transcribe.py --info

# Benchmark different models and worker counts
python3 benchmark_models.py --test-files 20 --models "tiny,base"

# Optimal parallel transcription
python3 parallel_transcribe.py --workers 8 --model base
```

### **üìä Performance Comparison**

| System | Sequential | Parallel (8 workers) | Speedup |
|--------|------------|---------------------|---------|
| M1 Max | 2-3 hours | 15-30 minutes | 4-8x faster |
| M2 Pro | 3-4 hours | 20-45 minutes | 4-6x faster |
| Intel i7 | 4-6 hours | 30-60 minutes | 3-5x faster |

### **üéØ Model Selection Guide**

**For Sequential Processing:**
- `tiny`: Fastest processing, 95% accuracy
- `base`: Good balance of speed and accuracy

**For Parallel Processing:**
- `base`: **Recommended** - Better CPU utilization
- `tiny`: May underutilize CPU cores

### **üîß Audio File Diagnostics**

```bash
# Check for problematic audio files
python3 check_audio_files.py

# Auto-fix common audio issues
python3 check_audio_files.py --fix

# Validate specific directory
python3 check_audio_files.py --dir audio_files_processed
```

### **üìà Benchmarking Tools**

```bash
# Test your system's optimal configuration
python3 benchmark_models.py

# Compare models with different worker counts
python3 benchmark_models.py --models "tiny,base,small" --max-workers 10

# Test specific file count
python3 benchmark_models.py --test-files 50
```

## üîß **Key Features**

### **Multiple User Interfaces**

1. **üåê Web Interface (Primary)**
   - `python3 app.py` ‚Üí `http://localhost:7860`
   - AI-powered Q&A with source attribution

2. **üíª Command Line Interface**
   - `python3 rag_pipeline.py "your question"`
   - `python3 search_transcripts.py "search terms"`

3. **ü§ñ Discord Bot**
   - `cd discord/ && python3 discord_bot.py`
   - Requires `DISCORD_TOKEN` in environment

### **AI Model Support**

- **OpenAI**: `gpt-4o`, `gpt-4`, `gpt-3.5-turbo`
- **Anthropic**: `claude-sonnet-4-20250514`, `claude-3-5-sonnet-20241022`
- **Automatic temperature fallback** for models that don't support it

### **Timestamp Accuracy**

- **Exact timestamps**: Video links jump to precise content location
- **Format**: `https://youtube.com/watch?v=VIDEO_ID&t=SECONDS`
- **Chunk precision**: Each chunk maps to specific video moments

## üõ†Ô∏è **Configuration**

### **Environment Variables**
```bash
# Required (at least one)
OPENAI_API_KEY=your_openai_key
ANTHROPIC_API_KEY=your_anthropic_key

# Optional
YOUTUBE_API_KEY=your_youtube_key
DISCORD_TOKEN=your_discord_token
```

### **Additional Dependencies for Parallel Processing**
```bash
# Install additional dependencies (already in requirements.txt)
pip install psutil torch
```

### **Key Settings** (in `pipeline_config.py`)
- **Chunk Size**: 250 words
- **Overlap**: 50 words
- **Vector Model**: `all-MiniLM-L6-v2`
- **Default Temperature**: 0.2

### **Parallel Processing Settings**
- **WHISPER_MODEL**: `"tiny"` (default for sequential processing)
- **WHISPER_MODEL_PARALLEL**: `"base"` (optimized for parallel processing)
- **PARALLEL_WORKERS**: `8` (default worker count)
- **PARALLEL_ENABLE**: `True` (enable parallel processing by default)

### **Model Performance Characteristics**
- **tiny**: 39M parameters, ~32x faster than base, 95% accuracy
- **base**: 74M parameters, reference speed, 99% accuracy
- **small**: 244M parameters, ~2x slower than base, 99.5% accuracy

## üéØ **Common Commands**

### **Basic Operations**
```bash
# Complete pipeline
python3 run_pipeline.py

# System validation
python3 validate_system.py

# Test RAG pipeline
python3 test_rag.py

# Search transcripts
python3 search_transcripts.py "covered calls" --top-k 5

# Ask questions
python3 rag_pipeline.py "What is theta decay?"
```

### **‚ö° Parallel Processing Commands**
```bash
# High-performance parallel transcription
python3 parallel_transcribe.py

# Custom parallel configuration
python3 parallel_transcribe.py --workers 6 --model base --gpu

# Fix timestamps with parallel processing
python3 fix_timestamp_issue.py --parallel --workers 8

# System performance analysis
python3 parallel_transcribe.py --info

# Benchmark your system
python3 benchmark_models.py --test-files 10
```

### **üîß Diagnostic Commands**
```bash
# Check audio file health
python3 check_audio_files.py

# Fix problematic audio files
python3 check_audio_files.py --fix

# Validate specific directory
python3 check_audio_files.py --dir audio_files_processed

# Resume interrupted processing
python3 parallel_transcribe.py --resume
```

## üìä **System Stats**

- **Videos**: ~500+ from Outlier Trading
- **Chunks**: ~14,000+ processed segments
- **Vector Store**: 60MB FAISS index
- **Query Time**: <2 seconds (including LLM generation)
- **Accuracy**: Exact video timestamps for navigation

### **‚ö° Performance Metrics**
- **Sequential transcription**: 2-6 hours for full dataset
- **Parallel transcription**: 15-60 minutes for full dataset (system dependent)
- **Optimal worker count**: 6-8 workers for most systems
- **Model recommendation**: `base` for parallel, `tiny` for sequential
- **Memory usage**: ~1-2GB per worker process

## üîß **Troubleshooting**

### **Common Issues**

1. **"ModuleNotFoundError"** ‚Üí Run setup commands in every new terminal
2. **"No timestamps in results"** ‚Üí Regenerate transcripts with `python3 fix_timestamp_issue.py --parallel`
3. **"API key not found"** ‚Üí Check `.env` file or environment variables
4. **"Vector store not found"** ‚Üí Run `python3 create_vector_store.py`
5. **"Failed to load audio"** ‚Üí Check audio files with `python3 check_audio_files.py --fix`
6. **"Transcription too slow"** ‚Üí Use parallel processing: `python3 parallel_transcribe.py`
7. **"Worker process crashed"** ‚Üí Reduce workers: `python3 parallel_transcribe.py --workers 4`

### **GitHub Actions Issues**

8. **"Workflow failed"** ‚Üí Check GitHub Actions logs for specific errors
9. **"No changes committed"** ‚Üí Workflow detected no new videos (expected behavior)
10. **"API key missing in workflow"** ‚Üí Add required secrets to GitHub repository settings:
    - `YOUTUBE_API_KEY` (required for video metadata)
11. **"Timeout in workflow"** ‚Üí Large processing jobs may take up to 3 hours
12. **"Hugging Face deployment failed"** ‚Üí Check `HF_TOKEN` secret and repository permissions
13. **"Vector store missing"** ‚Üí Vector store is created on Hugging Face during Docker build, not in GitHub Actions

### **Verification Commands**
```bash
# Check system health
python3 validate_system.py

# Test specific components
python3 test_single_question.py "test question"

# Check file counts
python3 count_files.py transcripts
python3 count_files.py processed_transcripts

# Check parallel processing capabilities
python3 parallel_transcribe.py --info

# Verify audio file health
python3 check_audio_files.py

# Test system performance
python3 benchmark_models.py --test-files 5
```

##  **Deployment**

### **Local Development**
```bash
# Docker (recommended)
./run_local.sh

# Python environment
source venv/bin/activate && python3 app.py
```

### **Production (Hugging Face)**
- Uses Docker for consistency
- Vector store built during image creation
- Automatic deployment via GitHub Actions

## üìö **Architecture**

```
YouTube Videos ‚Üí Video Discovery ‚Üí Transcript Generation ‚Üí 
Chunking & Metadata ‚Üí [GitHub Commit] ‚Üí Vector Store ‚Üí RAG Pipeline ‚Üí User Interface
        ‚Üë                                     ‚Üë                             ‚Üë
    GitHub Actions                     GitHub Actions                 Hugging Face
  (Steps 1-3 Weekly)                   (Commits & Triggers)         (Step 4 + Deployment)
```

**Key Components:**
- **FAISS Vector Store**: Semantic similarity search
- **Sentence Transformers**: Text embedding generation
- **OpenAI/Claude**: Answer generation
- **Gradio**: Web interface
- **Discord.py**: Bot interface
- **GitHub Actions**: Automated weekly processing workflow
- **Hugging Face Spaces**: Production hosting with auto-deployment

## üéâ **Success Indicators**

‚úÖ **Working System:**
- Video links include `&t=XXX` timestamps
- RAG answers include source attribution
- Search returns relevant results
- All validation checks pass
- GitHub Actions workflow runs successfully every Sunday
- New videos are automatically processed and deployed

‚ùå **Issues to Fix:**
- Links go to `&t=0` (timestamp issue)
- No source attribution in answers
- Empty search results
- Validation errors
- GitHub Actions workflow failures
- Missing API keys in workflow secrets

---

*For detailed troubleshooting and advanced configuration, see the development documentation in the repository.*
